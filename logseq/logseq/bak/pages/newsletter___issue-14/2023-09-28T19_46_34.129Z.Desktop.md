public:: true
tags:: [[newsletter]] [[AI]] 
categories:: [[newsletter]] 
coverimage:: /assets/Screenshot_2023-09-27_at_1.28.29_PM_1695857318835_0.png
date:: 2023-10-27
description:: Using machine learning to organize notes automatically and exploring them in a rich 3D interface
blogtitle:: Organizing Notes in 3D with AI

- Organizing notes and information is a challenge. Traditional methods like folders and tagging fall short because they require continuous manual effort to keep organized. What if we could leverage recent advances in machine learning to organize these notes automatically?
- At the same time, we have modern computing capable of rendering rich 3D environments. Yet, most user interfaces are still confined to a 2D plane.
- This post is an experiment that leverages machine learning text embeddings to organize Wikipedia articles in a 3D space within your browser.
- ![largesmall-autoplay.mp4](../assets/largesmall_1695859569049_0.mp4)
- ## Inspiration
	- A few notetaking apps like Logseq and Roam have a "graph view", which displays your notes on a 2D canvas, so you can explore connections between ideas at a high level.
	- Here's a screenshot of my graph view in Logseq
	- ![Screenshot 2023-09-27 at 12.40.49 PM.png](../assets/Screenshot_2023-09-27_at_12.40.49_PM_1695854486282_0.png){:height 613, :width 749}
	- This type of graph is called a "force directed layout". When I create a link between pages,  there's a line connecting those two page nodes. If I want to find topics related to Machine Learning, I can see which pages are connected.
	- This view is useful, but I wanted to extend it with two ideas:
		- Is there a way to automatically generate a view like this, without manually linking pages? How can we use machine learning to discover these relationships automatically?
		  logseq.order-list-type:: number
		- Can we display the nodes in 3D, to provide more insight into the relationships, and make it easier to explore large graphs with many nodes?
		  logseq.order-list-type:: number
- ## Intro to text embeddings
	- Text embeddings are lists of numbers generated by machine learning models that represent the "meaning" of textual data. You can generate text embeddings for short blocks of text, and compare their text embeddings to calculate their "similarity" to each other.
	- The text embedding models were trained on a large body of text, which allows them to understand the context and relationships between words and phrases. This enables algorithms to differentiate between words with multiple meanings, identify synonyms, and understand nuances, making the machine's interpretation closer to human understanding.
	- For example, "Text embeddings" could understand that the words "King" and "Queen" are similar, whereas "King" and "Dog" are not similar.
	- This technique has two popular uses: search and clustering.
	- In search, text embeddings help you find the page you search for, even when your search query doesn't directly match the content.  For example, if I searched "tallest mountain in the world", pages about "Mount Everest" would rank highly, even though my original search query didn't contain "Mount Everest".
	- In clustering, you group together similar pieces of text, which is the focus of this post.
- ## Text embeddings and UMAP
	- After turning text into embeddings, you can use an algorithm called UMAP to arrange the articles in either 2D or 3D space. We give the algorithm a list of embeddings, and it returns 2D or 3D coordinate points for each embedding.
	- In the visual representation, similar texts are grouped closely together.
- ## The Dataset
	- The experiment features two datasets: [Wikipedia's 1000 "Level 3 Vital Articles"](https://en.wikipedia.org/wiki/Wikipedia:Vital_articles) and [10,000 "Level 4 Vital Articles](https://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Level/4)."
	- These lists are articles that Wikipedia thinks are most essential. They're manually organized by Wikipedia editors into categories like history, people, geography, and science.
	- The demo has examples for the 1k and 10k sets of articles. Selecting the 10K option needs a reasonably powerful computer.
	- When generating the embeddings for each Wikipedia article, I break each wikipedia page into 500 word chunks, and average the embeddings for each page to calculate the "average meaning" of the page.
	- This same technique could be used for any type of document, such as notes or web pages, but Wikipedia is an interesting dataset for the demo.
- ## The Result
	- As expected, the text embeddings naturally cluster the Wikipedia articles into similar groups, mimicking their manual categorizations. In these visualizations I display the title of the wikipedia page, and position it based on the "meaning" of the text on that page.
	- ### 2D Space
		- Let's start by visualizing the articles in 2D space.
		- UMAP generates a graph of points in 2D space based on the embeddings, and we can see clusters of similar articles forming.
		- It's a bit hard to read the text in the clusters, but I didn't spend any time optimizing the spacing for the 2D demo.
		- ### Zoomed in
			- We can see articles about animals are grouped together
				- ![Screenshot 2023-09-27 at 11.42.44 AM.png](../assets/Screenshot_2023-09-27_at_11.42.44_AM_1695851450202_0.png)
			- Similarly, articles about food and ingredients are grouped together
				- ![Screenshot 2023-09-27 at 11.57.55 AM.png](../assets/Screenshot_2023-09-27_at_11.57.55_AM_1695851906326_0.png)
		- ### Zoomed out 2D Plot
			- ![Screenshot 2023-09-27 at 11.43.35 AM.png](../assets/Screenshot_2023-09-27_at_11.43.35_AM_1695851313702_0.png)
	- ### 3D Space
		- Now let's try 3D space. We just have to set n_components to 3 in UMAP to generate 3D points from the embeddings.
		- This view shows the articles in 3D, with colors based on location, to show groupings.The distant articles are points, but turn into text when you zoom in.
		- You can pan, rotate, and zoom around to explore the articles.
		- ![Screenshot 2023-09-27 at 1.28.29 PM.png](../assets/Screenshot_2023-09-27_at_1.28.29_PM_1695857318835_0.png)
		- We can zoom in to see groupings, and that similar articles are grouped together
		- ![Screenshot 2023-09-09 at 4.58.38 PM.png](../assets/Screenshot_2023-09-09_at_4.58.38_PM_1695860029294_0.png)
		- Subjectively, the groupings seem better and make more sense for the 3D view, compared to 2D. Turning the embeddings into 3D points, rather than 2D, seems to preserve much more information about their relationships.
		- The 3D space allows for more fluid categorizations, enabling articles to exist "between groups." For example, the "Philosophy of Science" article is placed in between a cluster of articles on science and a cluster of articles on philosophy.
- ## Implications and Uses
	- ### Dynamic Categorization
		- Organizing based on text embeddings allows for a dynamic, fluid way to categorize notes or articles. Unlike rigid folder structures, this method enables an article to belong to multiple overlapping categories or in between categories.
		- As new articles are added, all articles shift and reorganize themselves around the new articles, so no manual organization is necessary. It's automatically organized based on the meaning of the content
	- ### Discovery and Exploration
		- The 3D environment encourages users to explore information in a more engaging manner, potentially leading to unexpected yet relevant findings.
		- Since the collections are suggested by the machine and not formed manually, you're more likely to discover unexpected links and connections between information.
	- ### Semantic Search
		- Text embeddings are also used to implement "Semantic Search," which allows you to search for documents based on similarity instead of keywords.
		- The 3D gives us an intuitive understanding of how search works; we could imagine the search term in 3D space, and the search results would be the closest articles to the search query. The 3D graph could be a method of visualizing search results.
- ## Real-world Application
	- The primary focus of this experiment is Wikipedia. However, this method can extend to personal or corporate knowledge bases. It's a unique alternative for exploring and discovering information information.
	- With the potential rise in popularity of devices such as Apple Vision, the importance of crafting 3D interfaces could surge significantly.
- ## Example Python code
	- It's really simple to generate text embeddings and compare their similarity using Python. Here are some simplified examples so you can get a sense of how the code works.
	- Using the `all-MiniLM-L6-v2` model, you can give it text up to ~500 words longs, and it will generate a text embedding, which is an array of 384 floating point numbers.
	- ### Generate a text embedding
		- ``` python
		  from sentence_transformers import SentenceTransformer
		  model = SentenceTransformer('all-MiniLM-L6-v2')
		  
		  text_to_embed="The quick brown fox jumped over the lazy dog."
		  embedding = model.encode(subdocs_contents)
		  print(embeddings)
		  # [ 4.63508442e-02  9.21483636e-02  4.07040417e-02, ...]
		  ```
	- ### Compare similarity
		- Comparing text with high similarity
			- ``` python
			  from sentence_transformers.util import cos_sim
			  
			  similar_text = "The slow brown fox jumped over the lazy cat."
			  similar_embedding = model.encode(similar_text)
			  similar_score = cos_sim(embedding, similar_embedding)
			  print(similar_score)
			  # .9036 similarity. very similar
			  ```
		- Comparing text with low similarity
			- ```python
			  not_similar_text="Text embeddings help computers understand text better."
			  not_similar_embedding = model.encode(not_similar_text)
			  not_similar_score = cos_sim(embedding, not_similar_embedding)
			  print(not_similar)
			  # 0.0428 similary, not similar at all
			  ```
	- ### Turn embeddings into 2D or 3D points with UMAP
		- ```python
		  from umap import UMAP
		  
		  # n_components=3 gives us 3D points, =2 gives 2D points
		  xyz_points = UMAP(n_components=3).fit_transform(all_embeddings)
		  print(xyz_points)
		  # returns array of (x y z) coordinates, a 3d point for each piece of text
		  # [[0.3434,0.543, 0.0083],...]
		  ```
- ### Conclusion
	- The experiment shows the potential of combining text embeddings with 3D interfaces for information management. It not only simplifies the organization but also enriches the exploration experience.
	- As we continue to generate and consume more information, self-organizing approaches will be key to managing this ever-growing digital world.