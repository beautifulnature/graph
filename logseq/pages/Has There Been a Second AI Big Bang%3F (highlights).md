title:: Has There Been a Second AI Big Bang? (highlights)
author:: [[Calum Chace]]
full-title:: "Has There Been a Second AI Big Bang?"
category:: #articles
url:: https://www.forbes.com/sites/calumchace/2022/10/18/has-there-been-a-second-ai-big-bang/amp/

- Highlights first synced by [[Readwise]] [[Feb 20th, 2023]]
	- The Big Bang in artificial intelligence (AI) refers to the breakthrough in 2012, when a team of researchers led by Geoff Hinton managed to train an artificial neural network (known as a deep learning system) to win an image classification competition by a surprising margin ([View Highlight](https://read.readwise.io/read/01gg4evta8c5qpg6rztszk6xeg))
	- Has there been a new Big Bang in AI, since the arrival of Transformers in 2017? ([View Highlight](https://read.readwise.io/read/01gg4ewhckf1y451wxntwr0vt9))
	- Transformers are deep learning models which process inputs expressed in natural language and produce outputs like translations, or summaries of texts ([View Highlight](https://read.readwise.io/read/01gg4ewzm4krmftnt11y4q5nts))
	- paper titled “Attention is All You Need”. ([View Highlight](https://read.readwise.io/read/01gg4exf0fg985dyt1stgyjrh6))
	- Transformers can “pay attention” simultaneously to large corpus of text, whereas their predecessors, Recurrent Neural Networks, could only pay attention to the symbols either side of the segment of text being processed. ([View Highlight](https://read.readwise.io/read/01gg4exvejhggek8ydzjbjyf2z))
	- Transformers work by splitting text into small units, called tokens, and mapping them onto high-dimension networks - often thousands of dimensions ([View Highlight](https://read.readwise.io/read/01gg4ey355rxnyn6ah568hdd38))
	- We humans cannot envisage this. The space we inhabit is defined by three numbers – or four, if you include time, and we simply cannot imagine a space with thousands of dimensions. Researchers suggest that we shouldn’t even try. ([View Highlight](https://read.readwise.io/read/01gg4eye046vpz00gsy6598ff1))
	- For Transformer models, words and tokens have dimensions ([View Highlight](https://read.readwise.io/read/01gg4f01xhj8dz6b5y52h0sn34))
	- We might think of them as properties, or relationships. For instance, “man” is to “king” as “woman” is to “queen”. These concepts can be expressed as vectors, like arrows in three-dimensional space ([View Highlight](https://read.readwise.io/read/01gg4f09g121y6zm9v556mz2sc))
	- The model will attribute a probability to a particular token being associated with a particular vector. For instance, a princess is more likely to be associated with the vector which denotes “wearing a slipper” than to the vector that denotes “wearing a dog”. ([View Highlight](https://read.readwise.io/read/01gg4f0har62rzy1ysbgdye2gn))