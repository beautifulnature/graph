title:: Preventing an AI-related Catastrophe (highlights)
author:: [[Benjamin Hilton]]
full-title:: "Preventing an AI-related Catastrophe"
category:: #articles
url:: https://80000hours.org/problem-profiles/artificial-intelligence/#scaling

- Highlights first synced by [[Readwise]] [[Feb 20th, 2023]]
	- In all three surveys, the median researcher thought that the chances that AI would be “extremely good” was reasonably high: 20% in the 2016 survey, 20% in 2019, and 10% in 2022.[9](https://80000hours.org/problem-profiles/artificial-intelligence/#fn-9) ([View Highlight](https://read.readwise.io/read/01gfzbdz0gf695k8rsgbp8ms50))
	- A neural network transforms input data into output data by passing it through several hidden ‘layers’ of simple calculations, with each layer made up of ‘neurons.’ Each neuron receives data from the previous layer, performs some calculation based on its parameters (basically some numbers specific to that neuron), and passes the result on to the next layer. ([View Highlight](https://read.readwise.io/read/01gfzbfhmgtwgz00e0hgpn2qcj))
	- The engineers developing the network will choose some measure of success for the network (known as a [‘loss’ or ‘objective’ function](https://en.wikipedia.org/wiki/Loss_function)). The degree to which the network is successful (according to the measure chosen) will depend on the exact values of the parameters for each neuron on the network. ([View Highlight](https://read.readwise.io/read/01gfzbg7zrb404rg0b32z93t7e))
	- The network is then *trained* using a large quantity of data. By using an optimisation algorithm (most commonly [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)), the parameters of each neuron are gradually tweaked each time the network is tested against the data using the loss function. The optimisation algorithm will (generally) make the neural network perform slightly better each time the parameters are tweaked. Eventually, the engineers will end up with a network that performs pretty well on the measure chosen. ([View Highlight](https://read.readwise.io/read/01gfzbgnm1jt12x9jy14fchgxh))
	- •   Good algorithms (e.g. more efficient algorithms are better)
	  •   Data to train an algorithm
	  •   Enough computational power (known as *compute*) to do this training ([View Highlight](https://read.readwise.io/read/01gfzbk6b6az7py4h7q3f3asyw))
	- One option is to survey experts. Data from the [2019 survey of 300 AI experts](https://arxiv.org/abs/2206.04132) implies that there is 20% probability of human-level machine intelligence (which would plausibly be transformative in this sense) by 2036, 50% probability by 2060, and 85% by 2100.[21](https://80000hours.org/problem-profiles/artificial-intelligence/#fn-21) There are a lot of reasons to be suspicious of these estimates,[8](https://80000hours.org/problem-profiles/artificial-intelligence/#fn-8) but we take it as one data point. ([View Highlight](https://read.readwise.io/read/01gfzbnvbxe8x9tmfc6fk50xet))
	- Thinking through each step, **I think there’s [something like a 10% chance](https://80000hours.org/problem-profiles/artificial-intelligence/#how-likely-is-an-AI-related-catastrophe) of an existential catastrophe resulting from power-seeking AI systems this century.**[23](https://80000hours.org/problem-profiles/artificial-intelligence/#fn-23) ([View Highlight](https://read.readwise.io/read/01gfzbpjvwhj13wnbkbedqzcds))
	- **They have goals and are good at making plans.**
	  
	  Not all AI systems have goals or make plans to achieve those goals. But some systems (like some chess-playing AI systems) can be thought of in this way. When discussing power-seeking AI, we’re considering *planning* systems that are relatively advanced, with plans that are in pursuit of some goal(s), and that are capable of carrying out those plans. ([View Highlight](https://read.readwise.io/read/01gfzbq47032evmg9bv8p9pe79))
	- **They have excellent *strategic awareness*.**
	  
	  A particularly good planning system would have a good enough understanding of the world to notice obstacles and opportunities that may help or hinder its plans, and respond to these accordingly. Following [Carlsmith](https://arxiv.org/abs/2206.13353), we’ll call this *strategic awareness*, since it allows systems to strategise in a more sophisticated way ([View Highlight](https://read.readwise.io/read/01gfzbq8bqec1ppdzb3bc3m82d))
	- **They have highly *advanced capabilities* relative to today’s systems.**
	  
	  For these systems to actually affect the world, we need them to not just *make* plans, but also be good at all the specific tasks required to *execute* those plans.
	  
	  Since we’re worried about systems attempting to take power from humanity, we are particularly concerned about AI systems that might be better than humans on one or more tasks that grant *people* significant power when carried out well in today’s world. ([View Highlight](https://read.readwise.io/read/01gfzbqj6n17m40ca0a7cpbveg))
	- For example, people who are very good at persuasion and/or manipulation are often able to gain power — so an AI being good at these things might also be able to gain power. [Other examples](https://80000hours.org/articles/what-could-an-ai-caused-existential-catastrophe-actually-look-like/) might include hacking into other systems, tasks within scientific and engineering research, as well as business, military, or political strategy ([View Highlight](https://read.readwise.io/read/01gfzbqrnkvatck9612wpgnbf1))
	- What do we mean by “by default”? Essentially, **unless we actively find solutions to some (potentially quite difficult) problems, then it seems like we’ll create dangerously misaligned AI**. (There are reasons this might be wrong — which we discuss [later](https://80000hours.org/problem-profiles/artificial-intelligence/#best-arguments-against-this-problem-being-pressing)). ([View Highlight](https://read.readwise.io/read/01gfzbwes2e7d4a7zvtmsaeqbq))
	- Self-preservation — because a system is more likely to achieve its goals if it is still around to pursue them (in Stuart Russell’s [memorable phrase](https://80000hours.org/podcast/episodes/stuart-russell-human-compatible-ai/), “You can’t fetch the coffee if you’re dead ([View Highlight](https://read.readwise.io/read/01gfzbxr4hz835tt2ga2yvzz34))
	- Preventing any changes to the AI system’s goals — since changing its goals would lead to outcomes that are different from those it would achieve with its current goals ([View Highlight](https://read.readwise.io/read/01gfzby0f8a3v6dyfayepbyt6j))
	- Gaining power — for example, by getting more resources and greater capabilities. ([View Highlight](https://read.readwise.io/read/01gfzby3xvj738kvth0f8hgz4r))
	- **Control the objectives of the AI system.** We may be able to design systems that simply don’t have objectives to which the above argument applies — and thus don’t incentivise power-seeking behaviour. For example, we could find ways to explicitly instruct AI systems not to harm humans, or find ways to reward AI systems (in training environments) for not engaging in specific kinds of power-seeking behaviour (and also find ways to ensure that this behaviour continues outside the training environment). ([View Highlight](https://read.readwise.io/read/01gfzc157cbczrrafd571b91dw))
	- **Control the inputs into the AI system.** AI systems will only develop plans to seek power if they have enough information about the world to realise that seeking power is indeed a way to achieve its goals. ([View Highlight](https://read.readwise.io/read/01gfzc1zvt6peerbfqbwxgjcx4))
	- **Control the capabilities of the AI system.** AI systems will likely only be able to carry out plans to seek power if they have sufficiently [advanced capabilities](https://80000hours.org/problem-profiles/artificial-intelligence/#aps-systems) in skills that grant people significant power in today’s world ([View Highlight](https://read.readwise.io/read/01gfzc23w26y7rak0t02cxbytg))
	- we may be able to prevent power-seeking behaviour by ensuring that AI systems stop to check in with humans about any decisions they make. But these systems might be significantly slower and less immediately useful to people than systems that don’t stop to carry out these checks. As a result, there might still be incentives to use a faster, more initially effective misaligned system ([View Highlight](https://read.readwise.io/read/01gfzc3fsrc37kn7c2nfhq0zrz))
	- For example, if you’re developing an AI to improve military or political strategy, it’s much more useful if none of your rivals have a similarly powerful ([View Highlight](https://read.readwise.io/read/01gfzc9q3yyfvpdeyp6mp2tz9m))
	- By 2070 it will be possible and financially feasible to build strategically aware systems that can outperform humans on many power-granting tasks, and that can successfully make and carry out plans: Carlsmith guesses there’s a 65% chance of this being true ([View Highlight](https://read.readwise.io/read/01gfzdcnmcad2c3866cpdxp6ep))
	- **make future AI systems safe**, such as [Iterated Distillation and Amplification](https://www.youtube.com/watch?v=v9M2Ho9I9Qo), [AI safety via debate](https://www.alignmentforum.org/posts/LDsSqXf9Dpu3J3gHD/why-i-m-excited-about-debate), [building AI assistants that are uncertain about our goals and learn them by interacting with us](https://www.alignmentforum.org/posts/nd692YfFGfZDh9Mwz/an-69-stuart-russell-s-new-book-on-why-we-need-to-replace), or [finding ways to get AI systems trained with stochastic gradient descent to report truthfully what they know](https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge). For more, see Hubinger’s [11 possible proposals for building safe advanced AI](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai). ([View Highlight](https://read.readwise.io/read/01gfzdh4ksyztt12ccqzzxqh2p))
	- **Interpretability research** — trying to help us [actually understand what’s going on inside a neural network](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/). This work has had some success already (see, for example [*Zoom In: An Introduction to Circuits* by Olah et al.](https://distill.pub/2020/circuits/zoom-in/)). For more, see Hubinger’s [A transparency and interpretability tech tree](https://www.alignmentforum.org/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree) for an overview of this research, and Nanda’s [A Longlist of Theories of Impact for Interpretability](https://www.alignmentforum.org/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability) for an overview of how interpretability research could reduce existential risk from AI. ([View Highlight](https://read.readwise.io/read/01gfzdh9xdtjmb86w37gksa35k))
	- **Research to increase the robustness of neural networks** — making sure that the sorts of behaviour neural networks display when exposed to one set of inputs continues when exposed to inputs they haven’t previously been exposed to, in order to prevent AI systems [changing to unsafe behaviour](https://www.alignmentforum.org/posts/pDaxobbB9FG5Dvqyv/discussion-objective-robustness-and-inner-alignment) ([View Highlight](https://read.readwise.io/read/01gfzdhe3pgep9nx00vb1ym8bv))
	- [Anthropic](https://www.anthropic.com/) is an AI safety company working on building interpretable and safe AI systems. They focus on empirical AI safety research. Anthropic cofounders Daniela and Dario Amodei gave an [interview about the lab on the Future of Life Institute podcast](https://futureoflife.org/2022/03/04/daniela-and-dario-amodei-on-anthropic/). On our podcast, we spoke to [Chris Olah](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/), who leads Anthropic’s research into [interpretability](https://transformer-circuits.pub/), and [Nova DasSarma](https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/), who works on information security at Anthropic ([View Highlight](https://read.readwise.io/read/01gfzdpzg0s65ptk8p5f4x3pnd))
	- [DeepMind](https://deepmind.com/) is probably the largest and most well-known research group developing general artificialmachine intelligence, and is famous for its work creating [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo), [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero), and [AlphaFold](https://en.wikipedia.org/wiki/AlphaFold). It is not principally focused on safety, but has [two teams focused on AI safety](https://www.alignmentforum.org/posts/nzmCvRvPm4xJuqztv/deepmind-is-hiring-for-the-scalable-alignment-and-alignment). DeepMind is owned by Alphabet (Google’s parent company ([View Highlight](https://read.readwise.io/read/01gfzdq3tzcmsj7x8fzr5t9yfs))
	- The [Center for AI Safety](https://safe.ai/) is a nonprofit that does technical research and advocacy of machine learning safety in the wider academic community ([View Highlight](https://read.readwise.io/read/01gfzdq782qefqxh44f4s0v789))
	- [OpenAI](https://openai.com/), founded in 2015, is a lab that is trying to build artificial general intelligence that is safe and benefits all of humanity. OpenAI is well known for its language models like [GPT-3](https://en.wikipedia.org/wiki/GPT-3), and has a safety team and a governance team. Jan Leike (head of the alignment team) has some blog posts on [how he thinks about AI alignment](https://aligned.substack.com/). ([View Highlight](https://read.readwise.io/read/01gfzdqa2xt3mrb4hkv0b6x6pf))
	- [Ought](https://ought.org/) is a machine learning lab building [Elicit](https://elicit.org), an AI research assistant. Their aim is to align open-ended reasoning by [learning human reasoning steps](https://ought.org/updates/2022-04-06-process), and to direct AI progress towards helping with [evaluating evidence and arguments](https://ought.org/updates/2022-04-08-elicit-plan) ([View Highlight](https://read.readwise.io/read/01gfzdqd4x8b9n8d9wny6xptqq))
	- [Redwood Research](https://www.redwoodresearch.org/) is an AI safety research organisation, whose [first big project](https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project) attempted to make sure language models (like GPT-3) produce output following certain rules with very high probability, in order to address failure modes too rare to show up in standard training. ([View Highlight](https://read.readwise.io/read/01gfzdqjgx8gwyy9n4azvjnmg7))
	- The [Alignment Research Center](https://alignment.org/) (ARC) is attempting to produce alignment strategies that could be adopted in industry today while also being able to scale to future systems. They focus on conceptual work, developing strategies that could work for alignment and which may be promising directions for empirical work, rather than doing empirical AI work themselves. Their first project was releasing a report on [Eliciting Latent Knowledge](https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge), the problem of getting advanced AI systems to honestly tell you what they believe (or ‘believe’) about the world. On our podcast, we interviewed [ARC founder Paul Christiano about his](https://80000hours.org/podcast/episodes/paul-christiano-a-message-for-the-future/) ([View Highlight](https://read.readwise.io/read/01gfzdqwb4yfw2gh1djj2n5eaz))
	- The [Center on Long-Term Risk](https://longtermrisk.org/) works to address worst-case risks from advanced AI. They focus on conflict between AI systems. ([View Highlight](https://read.readwise.io/read/01gfzdr1gz2ranb7afnqm0f1v1))
	- The [Machine Intelligence Research Institute](https://intelligence.org/) was one of the first groups to become concerned about the risks from machine intelligence in the early 2000s, and its team has published a [number of papers](https://intelligence.org/research/) on safety issues and how to resolve them. ([View Highlight](https://read.readwise.io/read/01gfzdr5gw7y19h9wszg8k1p4x))
	- academia:
	  
	  •   The Algorithmic Alignment Group in the Computer Science and Artificial Intelligence Laboratory at MIT, led by [Dylan Hadfield-Menell](https://people.csail.mit.edu/dhm/) ([View Highlight](https://read.readwise.io/read/01gfzdrm80ncfz1mf0tgmpne4k))
	- The [Center for Human-Compatible AI](https://humancompatible.ai/) at UC Berkeley, led by Stuart Russell, which focuses on academic research to ensure AI is safe and beneficial to humans. (Our [podcast with Stuart Russell](https://80000hours.org/podcast/episodes/stuart-russell-human-compatible-ai/) examines his approach to provably beneficial AI ([View Highlight](https://read.readwise.io/read/01gfzdrq1x3r74wq9qqf00aw95))
	- [Jacob Steinhardt’s research group](https://jsteinhardt.stat.berkeley.edu/) in the Department of Statistics at UC Berkeley ([View Highlight](https://read.readwise.io/read/01gfzdrt27p5herzkpme82vgyc))
	- David Krueger’s research group at the [Computational and Biological Learning Laboratory](http://www.eng.cam.ac.uk/research/academic-divisions/information-engineering/research-groups/computational-and-biological) at the University of Cambridge. ([View Highlight](https://read.readwise.io/read/01gfzdrwvze2wa7d27a65vgqmb))
	- The [Future of Humanity Institute](https://www.fhi.ox.ac.uk/) at the University of Oxford has an [AI safety research group](https://www.fhi.ox.ac.uk/research/research-areas/#aisafety_tab). ([View Highlight](https://read.readwise.io/read/01gfzdrzye08h4v468ff0cwt6f))
	- If you’re interested in learning more about technical AI safety as an area — e.g. the different techniques, schools of thought, and threat models — our top recommendation is to take a look at the [technical alignment curriculum](https://www.eacambridge.org/technical-alignment-curriculum) from Effective Altruism Cambridge ([View Highlight](https://read.readwise.io/read/01gfzds5n7xpvd8e8ecq8trvqa))
	- [AI Impacts](https://aiimpacts.org/) attempts to find answers to all sorts of relevant questions about the future of AI, like [“How likely is a sudden jump in AI progress at around human-level performance?”](https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/) ([View Highlight](https://read.readwise.io/read/01gfzdsschrkm3wp8c8tw7csxb))
	- The [AI Security Initiative](https://cltc.berkeley.edu/ai-security-initiative/) at UC Berkeley’s Center ([View Highlight](https://read.readwise.io/read/01gfzdsw3zbr4dycb6z4ejgq9v))
	- [AI could defeat all of us combined](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/) and [the “most important century” blog post series](https://www.cold-takes.com/most-important-century/) by Holden Karnofsky, co-CEO of Open Philanthropy, argues that the 21st century could be the most important century ever for humanity as a result of AI. ([View Highlight](https://read.readwise.io/read/01gfzdv5j6hwxvp7x68n3am4s5))
	- [Why AI alignment could be hard with modern deep learning](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/) by Open Philanthropy researcher Cotra is a gentle introduction to how risks from power-seeking AI could play out with current machine learning methods. [Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover](https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to), also by Cotra, provides a much more detailed description of how risks could play out (which we’d recommend for people familiar with ML ([View Highlight](https://read.readwise.io/read/01gfzdva0sca55fp26bszmx5f4))
	- [AGI safety from first principles](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ) provides OpenAI governance researcher Richard Ngo’s perspective on how to think about risks from artificial general intelligence ([View Highlight](https://read.readwise.io/read/01gfzdvf93rk5xxpk4spj7kbs1))
	- [Is power-seeking AI an existential risk?](https://arxiv.org/abs/2206.13353) by Open Philanthropy researcher Joseph Carlsmith is an in-depth look covering exactly how and why AI could cause the disempowerment of humanity (but watch out — it’s even longer than this article!). For a shorter summary, see Carlsmith’s [talk on the same topic](https://forum.effectivealtruism.org/posts/ChuABPEXmRumcJY57/video-and-transcript-of-presentation-on-existential-risk). ([View Highlight](https://read.readwise.io/read/01gfzdvjsepa6pfsf87bdchyc1))
	- [Distinguishing AI takeover scenarios](https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios) by Sam Clarke and Sammy Martin summarises various ways in which AI could go wrong. ([View Highlight](https://read.readwise.io/read/01gfzdvrd4fzxbxf6wkkd4vjse))
	- [AI governance: Opportunity and theory of impact](https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact) by DeepMind governance lead Allan Dafoe explores ways in which research into AI governance could effect change ([View Highlight](https://read.readwise.io/read/01gfzdvvrezmq3vmqhd94324es))
	- •   by DeepMind governance lead Allan Dafoe explores ways in which research into AI governance could effect change.
	  •   [A bird’s-eye view of the AI alignment landscape](https://www.alignmentforum.org/posts/SQ9cZtfrzDJmw9A2m/my-overview-of-the-ai-alignment-landscape-a-bird-s-eye-view) by Neel Nanda summarises the different ways in which technical alignment research could reduce the risk from AI. ([View Highlight](https://read.readwise.io/read/01gfzdvyst66eknq5ah5btxzcm))
	- [An overview of 11 proposals for building safe advanced AI](https://arxiv.org/abs/2012.07532) by Evan Hubinger discusses and evaluates plausible techniques for AI alignment ([View Highlight](https://read.readwise.io/read/01gfzdw1zy3kdn4q0m3ay4avha))
	- Podcasts: The [*AI X-risk Research Podcast*](https://axrp.net/), particularly [episode 12 with Paul Christiano](https://axrp.net/episode/2021/12/02/episode-12-ai-xrisk-paul-christiano.html) and [episode 13 with Richard Ngo](https://axrp.net/episode/2022/03/31/episode-13-first-principles-agi-safety-richard-ngo.html) — both of which serve as excellent introductions to AI risk ([View Highlight](https://read.readwise.io/read/01gfzdw4xyepsqmb06hsecpbw3))
	- [Paul Christiano](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/) on his vision of how humanity might progressively hand over decision-making to AI systems. ([View Highlight](https://read.readwise.io/read/01gfzdw8knfkfs6apwfe99evxg))
	- •   [Allan Dafoe](https://80000hours.org/podcast/episodes/allan-dafoe-politics-of-ai/) on trying to prepare the world for the possibility that AI will destabilise global politics.
	  •   [Dario](https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/) ([View Highlight](https://read.readwise.io/read/01gfzdwbmdb48zb3ftn08m7jyc))
	- [Dario Amodei](https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/) of Anthropic explains how to become an AI researcher ([View Highlight](https://read.readwise.io/read/01gfzdwfgavgg32ts1vt127vvc))
	- [Miles Brundage](https://80000hours.org/2017/06/the-world-desperately-needs-ai-strategists-heres-how-to-become-one) of OpenAI explains how to become an AI strategist ([View Highlight](https://read.readwise.io/read/01gfzdwjc468cybnzwfqe8q1v8))
	- Holden Karnofsky, cofounder of GiveWell and Open Philanthropy has been on two of our podcasts: [he explains how philanthropy can have maximum impact by taking big risks](https://80000hours.org/2018/02/holden-karnofsky-open-philanthropy/) (including a discussion of his work in positively shaping the development of AI), and he discusses [why this might be the most important century](https://80000hours.org/podcast/episodes/holden-karnofsky-most-important-century/). ([View Highlight](https://read.readwise.io/read/01gfzdwne14zm4zdrqdactjk3h))
	- PhD or programming? Fast paths into aligning AI as a machine learning engineer, [according to ML engineers Catherine Olsson and Daniel Ziegler](https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety/). ([View Highlight](https://read.readwise.io/read/01gfzdwrwebbxyrtbtpj84cykf))
	- [Jan Leike](https://80000hours.org/2018/03/jan-leike-ml-alignment/) (now head of the Alignment team at OpenAI) explains how to become a machine learning alignment researcher. ([View Highlight](https://read.readwise.io/read/01gfzdwvb9n93gx3c0gpyq54sh))