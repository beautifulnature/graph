- [[multivariate linear regression]]
- [[Deeplearning.ai]]
	- github links
	  collapsed:: true
		- https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera
		- https://github.com/greyhatguy007/deep-learning-specialization
		- https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization
		- https://github.com/amanchadha/coursera-gan-specialization
		- https://github.com/amanchadha/coursera-machine-learning-engineering-for-prod-mlops-specialization
		- https://github.com/mattborghi/mlops-specialization
		- https://github.com/honghanhh/coursera-practical-data-science-specialization
		- https://github.com/williamcwi/DeepLearning.AI-TensorFlow-Developer-Professional-Certificate
	- courses
	  collapsed:: true
		- machine learning
		- mathematiCs for machine learning
		- deep learning specialization
		- natural language processing specialization
		- tensorflow professional
		- ml ops
		- data science aws cloud
- [[deeplearning.ai/Supervised Learning/multiple-linear-regression]]
	- # [[Gradient descent]] for multiple linear regression
	  id:: 64069af3-03bb-462f-bb8d-cd9addc15804
		- Remember for one feature gradient descent
		  collapsed:: true
			- Update w and b simultaneously and repeat
			- ((63fc17d1-9115-416c-8f28-6c35f4fa06b4))
			- ((0cfb0049-ba52-47b3-8f43-4da56b3c0832))
		- ## Multiple linear regression gradient descent
			- id:: 63fc3fab-4eb6-4ab7-ada2-3eb3bb8afee1
			  $$\begin{align*} \text{repeat}&\text{ until convergence:} \; \lbrace \newline\;
			  & w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j}  \; & \text{for j = 0..n-1}\newline
			  &b\ \ = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}  \newline \rbrace
			  \end{align*}$$
			- where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously
			  id:: 5cb7e36f-aee4-41ae-baff-90dc232b8758
			- and where:
			  id:: 63fc4082-067a-4c88-a825-10771bedd7e5
			- id:: 63fc3fea-0246-484d-9864-8e7768ab1249
			  $$\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}$$
			- id:: 63fc4027-4f0a-474d-aa4a-ff8edb89c09e
			- id:: 63fc4020-ea05-48e3-9443-1098de3f4f30
			  $$\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) $$
			- m is the number of training examples in the data set
			- $f_{\mathbf{w},b}(\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value
			- ### Fully expanded
				- $$w_1=w_1-\alpha  \frac{1}{n}\sum_{i=1}^{n}(f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})x_1^{(i)}$$
				- Update all the w's from 1 to n
				-
				- $$b=b-\alpha\frac{1}{n}\sum_{i=1}^{n}(f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})$$
				- Simultaneously update both and repeat
			- ### Normal Equation
				- An alternative way to find w and b for linear regression
				- This works only for linear regression
				- Solves for w and b to minimize cost
				- Can use a linear algebra library to solve directly
				- Doesn't generalize other learning algorithms
				- slow when features is large (> 10000)
				- Normal equation may be used in libraries to  implement linear regression
				- Gradient descent is the reccomended way to find w and b
	- # Multiple linear regression lab
	  collapsed:: true
		- ## Notation
		  id:: 63fc1c69-b02a-404d-a109-2c12f4de508b
			- ### General Notation
			  id:: 63fc1d44-7566-42ef-85b3-0f2cdb97a18e
				- id:: 63fc1c56-34ab-425c-8520-30f954a60759
				  |  Notation   | Description | Python (if applicable) |
				  |: ------------|: ------------------------------------------------------------||
				  | $a$ | scalar, non bold                                                      ||
				  | $\mathbf{a}$ | vector, bold                                                 ||
				  | $\mathbf{A}$ | matrix, bold capital                                         ||
			- ### Regression Notation
			  id:: 63fc1d38-780d-4a81-a4b9-6244744b1a06
				- id:: 63fc1d12-ebee-4268-b776-8caa7adaa901
				  |  Notation   | Description | Python (if applicable) |
				  |  $\mathbf{X}$ | training example matrix                  | `X_train` |   
				  |  $\mathbf{y}$  | training example  targets                | `y_train` |
				  |  $\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|
				  | m | number of training examples | `m`|
				  | n | number of features in each example | `n`|
				  |  $\mathbf{w}$  |  parameter: weight,                       | `w`    |
				  |  $b$           |  parameter: bias                                           | `b`    |     
				  | $f_{\mathbf{w},b}(\mathbf{x}^{(i)})$ | The result of the model evaluation at $\mathbf{x^{(i)}}$ parameterized by $\mathbf{w},b$: $f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)}+b$  | `f_wb` |
		- The example is predicting housing prices
		- | Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   
		  | ----------------| ------------------- |----------------- |--------------|-------------- |  
		  | 2104            | 5                   | 1                | 45           | 460           |  
		  | 1416            | 3                   | 2                | 40           | 232           |  
		  | 852             | 2                   | 1                | 35           | 178           |
		- ```python
		  X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])
		  y_train = np.array([460, 232, 178])
		  ```
		- ## Matrix Containing Examples
			- Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).
			  
			  
			  $$\mathbf{X} = 
			  \begin{pmatrix}
			   x^{(0)}_0 & x^{(0)}_1 & \cdots & x^{(0)}_{n-1} \\ 
			   x^{(1)}_0 & x^{(1)}_1 & \cdots & x^{(1)}_{n-1} \\
			   \cdots \\
			   x^{(m-1)}_0 & x^{(m-1)}_1 & \cdots & x^{(m-1)}_{n-1} 
			  \end{pmatrix}
			  $$
			  notation:
			- $\mathbf{x}^{(i)}$ is vector containing example i. $\mathbf{x}^{(i)} = (x^{(i)}_0, x^{(i)}_1, \cdots,x^{(i)}_{n-1})$
			- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.
		- ## Parameter vector w, b
		  
		  * $\mathbf{w}$ is a vector with $n$ elements.
			- Each element contains the parameter associated with one feature.
			- in our dataset, n is 4.
			- notionally, we draw this as a column vector
			  
			  $$\mathbf{w} = \begin{pmatrix}
			  w_0 \\ 
			  w_1 \\
			  \cdots\\
			  w_{n-1}
			  \end{pmatrix}
			  $$
			- $b$ is a scalar parameter.
		- ```python
		  b_init = 785.1811367994083
		  w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])
		  ```
	- ## Model Prediction With Multiple Variables
	  collapsed:: true
		- The model's prediction with multiple variables is given by the linear model:
		  
		  $$ f_{\mathbf{w},b}(\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \tag{1}$$
		- or in vector notation:
		- $$ f_{\mathbf{w},b}(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b  \tag{2} $$
		- where $\cdot$ is a vector `dot product`
	- ## For loop prediction
	  collapsed:: true
		- loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end.
		- ```python
		  def predict_single_loop(x, w, b): 
		      """
		      single predict using linear regression
		      
		      Args:
		        x (ndarray): Shape (n,) example with multiple features
		        w (ndarray): Shape (n,) model parameters    
		        b (scalar):  model parameter     
		        
		      Returns:
		        p (scalar):  prediction
		      """
		      n = x.shape[0]
		      p = 0
		      for i in range(n):
		          p_i = x[i] * w[i]  
		          p = p + p_i         
		      p = p + b                
		      return p
		  ```
	- # Numpy Vector Single Prediction
	  collapsed:: true
		- ```python
		  def predict(x, w, b): 
		      """
		      single predict using linear regression
		      Args:
		        x (ndarray): Shape (n,) example with multiple features
		        w (ndarray): Shape (n,) model parameters   
		        b (scalar):             model parameter 
		        
		      Returns:
		        p (scalar):  prediction
		      """
		      p = np.dot(x, w) + b     
		      return p  
		  ```
	- # Compute cost with multiple variables
		- The equation for the [[cost function]] with multiple variables $J(\mathbf{w},b)$ is:
		  id:: 64069af3-f9a4-476b-8ccd-9928e3f730e3
		- id:: 64069af3-9b51-4efc-8ad3-ce3006a38432
		  $$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 \tag{3}$$
		- where:
		  id:: 64069af3-4639-44f4-b223-9e0589365480
		- id:: 6406f79f-e73d-4ef5-b148-eb8b84232129
		  $$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b  \tag{4} $$
		- Compared the predicted values using vectors
		- In contrast to previous labs, $\mathbf{w}$ and $\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features.
		- ```python
		  def compute_cost(X, y, w, b): 
		      """
		      compute cost
		      Args:
		        X (ndarray (m,n)): Data, m examples with n features
		        y (ndarray (m,)) : target values
		        w (ndarray (n,)) : model parameters  
		        b (scalar)       : model parameter
		        
		      Returns:
		        cost (scalar): cost
		      """
		      m = X.shape[0]
		      cost = 0.0
		      for i in range(m):                                
		          f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)
		          cost = cost + (f_wb_i - y[i])**2       #scalar
		      cost = cost / (2 * m)                      #scalar    
		      return cost
		  ```
- # Compute Gradient with multiple variables
	- An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an
	- outer loop over all m examples.
		- $\frac{\partial J(\mathbf{w},b)}{\partial b}$ for the example can be computed directly and accumulated
		- in a second loop over all n features:
			- $\frac{\partial J(\mathbf{w},b)}{\partial w_j}$ is computed for each $w_j$.
	- ```python
	  def compute_gradient(X, y, w, b): 
	      """
	      Computes the gradient for linear regression 
	      Args:
	        X (ndarray (m,n)): Data, m examples with n features
	        y (ndarray (m,)) : target values
	        w (ndarray (n,)) : model parameters  
	        b (scalar)       : model parameter
	        
	      Returns:
	        dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
	        dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. 
	      """
	      m,n = X.shape           #(number of examples, number of features)
	      dj_dw = np.zeros((n,))
	      dj_db = 0.
	  
	      for i in range(m):                             
	          err = (np.dot(X[i], w) + b) - y[i]   
	          for j in range(n):                         
	              dj_dw[j] = dj_dw[j] + err * X[i, j]    
	          dj_db = dj_db + err                        
	      dj_dw = dj_dw / m                                
	      dj_db = dj_db / m                                
	          
	      return dj_db, dj_dw
	  ```
	- # Gradient Descent
		- ```python
		  def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): 
		      """
		      Performs batch gradient descent to learn w and b. Updates w and b by taking 
		      num_iters gradient steps with learning rate alpha
		      
		      Args:
		        X (ndarray (m,n))   : Data, m examples with n features
		        y (ndarray (m,))    : target values
		        w_in (ndarray (n,)) : initial model parameters  
		        b_in (scalar)       : initial model parameter
		        cost_function       : function to compute cost
		        gradient_function   : function to compute the gradient
		        alpha (float)       : Learning rate
		        num_iters (int)     : number of iterations to run gradient descent
		        
		      Returns:
		        w (ndarray (n,)) : Updated values of parameters 
		        b (scalar)       : Updated value of parameter 
		        """
		      
		      # An array to store cost J and w's at each iteration primarily for graphing later
		      J_history = []
		      w = copy.deepcopy(w_in)  #avoid modifying global w within function
		      b = b_in
		      
		      for i in range(num_iters):
		  
		          # Calculate the gradient and update the parameters
		          dj_db,dj_dw = gradient_function(X, y, w, b)   ##None
		  
		          # Update Parameters using w, b, alpha and gradient
		          w = w - alpha * dj_dw               ##None
		          b = b - alpha * dj_db               ##None
		        
		          # Save cost J at each iteration
		          if i<100000:      # prevent resource exhaustion 
		              J_history.append( cost_function(X, y, w, b))
		  
		          # Print cost every at intervals 10 times or as many iterations if < 10
		          if i% math.ceil(num_iters / 10) == 0:
		              print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   ")
		          
		      return w, b, J_history #return final w,b and J history for graphing
		  ```
		- ## Usage
			- ```python
			  # initialize parameters
			  initial_w = np.zeros_like(w_init)
			  initial_b = 0.
			  # some gradient descent settings
			  iterations = 1000
			  alpha = 5.0e-7
			  # run gradient descent 
			  w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,
			                                                      compute_cost, compute_gradient, 
			                                                      alpha, iterations)
			  print(f"b,w found by gradient descent: {b_final:0.2f},{w_final} ")
			  m,_ = X_train.shape
			  for i in range(m):
			      print(f"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}")
			  ```