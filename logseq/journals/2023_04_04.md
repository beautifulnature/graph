- [[deeplearning.ai/Supervised Learning]]
- [[Deeplearning.ai/Machine Learning Specialization]]
	- To compute the partial derivative of the squared error cost function J(w, b) with respect to w, we will first break down the components of the function and then apply the rules of differentiation.
	  
	  Given the cost function:
	  
	  $J(w,b) = \frac{1}{2n}\sum_{i=1}^{n}(f_{w,b}(x^{(i)}) - y^{(i)})^2$
	  
	  Where:
	- J(w, b) is the cost function.
	- n is the number of data points.
	- f_{w,b}(x^{(i)}) is the predicted output for the i-th data point, which is given by the linear regression function f(x) = wx + b.
	- x^{(i)} is the i-th input data point.
	- y^{(i)} is the i-th actual output data point.
	  
	  The first step is to take the partial derivative of the cost function with respect to w. We will apply the chain rule to compute this derivative:
	  
	  $\frac{\partial J(w, b)}{\partial w} = \frac{1}{2n} \sum_{i=1}^{n} 2(f_{w,b}(x^{(i)}) - y^{(i)}) \frac{\partial}{\partial w}(f_{w,b}(x^{(i)}) - y^{(i)})$
	  
	  Notice that the factor 2 in the derivative cancels out with the 1/2 in the cost function, simplifying the expression:
	  
	  $\frac{\partial J(w, b)}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} (f_{w,b}(x^{(i)}) - y^{(i)}) \frac{\partial}{\partial w}(f_{w,b}(x^{(i)}) - y^{(i)})$
	  
	  Now we need to compute the partial derivative of the inner term (f_{w,b}(x^{(i)}) - y^{(i)}) with respect to w:
	  
	  $\frac{\partial}{\partial w}(f_{w,b}(x^{(i)}) - y^{(i)}) = \frac{\partial}{\partial w}(wx^{(i)} + b - y^{(i)})$
	  
	  Since y^{(i)} and x^{(i)} are constants with respect to w, their partial derivatives are zero. Therefore, we have:
	  
	  $\frac{\partial}{\partial w}(wx^{(i)} + b - y^{(i)}) = x^{(i)}$
	  
	  Substitute this back into the partial derivative of the cost function:
	-
	- $\frac{\partial J(w, b)}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}$
	  
	  This is the partial derivative of the squared error cost function with respect to w. The summation term represents the sum of the product of the error (the difference between the predicted and actual output) and the input value x^{(i)} for all data points.
	- The chain rule states that if you have a composite function, such as h(x) = g(f(x)), and you want to find its derivative, you would compute the derivative of g with respect to f, multiplied by the derivative of f with respect to x.
	- In our case, the cost function J(w, b) contains a composite function:
	  $J(w,b) = \frac{1}{2n}\sum_{i=1}^{n}(f_{w,b}(x^{(i)}) - y^{(i)})^2$
	- To compute the partial derivative with respect to w, we'll apply the chain rule to the square term in the cost function. Let's denote the inner term as u:
	  $u = (f_{w,b}(x^{(i)}) - y^{(i)})$
	- Now the cost function can be written as:
	- $J(w,b) = \frac{1}{2n}\sum_{i=1}^{n}u^2$
	- Applying the chain rule to find the partial derivative of J(w, b) with respect to w:
	- $\frac{\partial J(w, b)}{\partial w} = \frac{1}{2n} \sum_{i=1}^{n} 2u \frac{\partial u}{\partial w}$
	- In this step, we are first taking the derivative of the squared term with respect to u (which is 2u), then multiplying it by the partial derivative of u with respect to w (which we have yet to compute).
	- The factor 2 in the derivative cancels out with the 1/2 in the cost function, which simplifies the expression:
	- $\frac{\partial J(w, b)}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} u \frac{\partial u}{\partial w}$
	- Now, our next step is to compute the partial derivative of u (the inner term) with respect to w and substitute it back into the expression.
	- Recall that:
	  $u = (f_{w,b}(x^{(i)}) - y^{(i)})$
	- And the linear regression function is:
	- $f_{w,b}(x^{(i)}) = wx^{(i)} + b$
	- Substitute the linear regression function into the expression for u:
	  $u = (wx^{(i)} + b - y^{(i)})$
	- Now, compute the partial derivative of u with respect to w:
	- $\frac{\partial u}{\partial w} = \frac{\partial}{\partial w}(wx^{(i)} + b - y^{(i)})$
	- Since y^{(i)} and x^{(i)} are constants with respect to w, and b is independent of w, their partial derivatives are zero. Thus, the partial derivative of u with respect to w simplifies to:
	- $\frac{\partial u}{\partial w} = x^{(i)}$
	- Now, substitute the computed partial derivative of u back into the expression for the partial derivative of J(w, b) with respect to w:
	- $\frac{\partial J(w, b)}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} u x^{(i)}$
	- Replace u with the original expression:
	  $\frac{\partial J(w, b)}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}$
	- This is the partial derivative of the squared error cost function with respect to w. The summation term represents the sum of the product of the error (the difference between the predicted and actual output) and the input value x^{(i)} for all data points.
	- With this partial derivative, we can apply gradient descent or other optimization techniques to minimize the cost function J(w, b) and find the optimal values for the parameters w and b in the linear regression model.
- [[linear regression]]
	-