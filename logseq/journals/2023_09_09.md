- [[newsletter/issue-14]]
	- Organizing notes and information is a challenge. Traditional methods like tagging and folders fall short, because they require manual effort. What if we could leverage machine learning to automatically categorize these notes? Recent advances in AI "Large Language Models" and "Text embeddings" make it possible to do this easily and accurately.
	- At the same time, we have modern computing capable of rendering rich 3D environments. Yet, most application interfaces are still confined to a 2D plane.
	- This blog post delves into an experiment that leverages machine learning text embeddings to organize Wikipedia articles in a 3D space within your browser.
	- ### Text Embeddings: A Primer
		- Text embeddings are lists of numbers generated by machine learning algorithms to capture the "semantic meaning" of textual data
		- In the context of machine learning, capturing semantic meaning involves understanding the context in which words or phrases are used within the text. This enables algorithms to differentiate between words with multiple meanings, identify synonyms, and understand nuances, making the machine's interpretation of text closer to human understanding.
		- For example, "Text embeddings" could understand that the words "King" and "Queen" are similar, whereas "King" and "Dog" are not similar.
	- ### Text embeddings in 3D
		- In this experiment, the wikipedia articles are represented by lists of 385 numbers.  Using an algorithm called U-MAP, we can turn these lists of 385 numbers into lists of length 2 or 3, which we can plot in 3d.
	- ### The Experiment: Wikipedia in 3D
	- #### The Dataset
	- The experiment features two datasets: Wikipedia's 1000 "Level 3 Vital Articles" and 10,000 "Level 4 Vital Articles." These lists comprise articles that Wikipedia deems most important and are manually categorized into various subjects like history, people, geography, and science.
	- #### The 3D Space
	- In this 3D environment, articles that are close to each other are considered "similar" based on their text embeddings. This offers a unique way to serendipitously explore information by simply zooming through this 3D space.
	- #### The Result
	- As expected, the text embeddings naturally cluster the Wikipedia articles into similar groups, mimicking their manual categorizations. However, the 3D space allows for more fluid categorizations, enabling articles to exist "in between groups." For instance, the article on "Philosophy of Science" finds its place between clusters of articles on science and philosophy.
	- ### Implications and Uses
	- #### Dynamic Categorization
	- The 3D space allows for a dynamic, fluid way to categorize notes or articles. Unlike rigid folder structures, this method enables an article to belong to multiple, overlapping categories.
	- #### Discovery and Exploration
	- The 3D environment invites users to explore information in a more engaging manner, potentially leading to unexpected yet relevant findings.
	- #### Real-world Application
	- While this experiment focuses on Wikipedia, the methodology can be applied to personal or corporate note-taking systems, providing a more intuitive and efficient way to manage information.
	- ### Conclusion
	- The experiment shows the untapped potential of combining text embeddings with 3D interfaces for information management. It not only simplifies the organization but also enriches the exploration and retrieval experience. As we continue to generate and consume more information, such innovative approaches could be the key to managing this ever-growing digital landscape.
	- second draft
	  collapsed:: true
		- Organizing and retrieving notes can be a challenge. Conventional methods like tagging and folders only go so far and require manual effort. This experiment uses machine learning "text embeddings" to automatically organize notes based on meaning.
		- Also, while modern computers have the capability to render rich 3D environments, even in browsers, the UI design of most apps remains staunchly 2D.
		- ## Wikipedia
			- This experiment uses machine learning text embeddings to organize Wikipedia pages and display them in the browser in 3D.
			- Articles close together in 3D space are considered "similar" to each other. This is an interesting way to serendipitously explore information, discovering related articles are by zooming around in 3D space.
			- The demo displays two graphs, Wikipedia's 1000 "Level 3 Vital Articles" and 10,000 "Level 4 Vital Articles"
			- These are lists of what Wikipedia considers its most important articles, and is an interesting and rich example. These articles are manually organized into categories like "history", "People", "Geography", "Science", etc.
			- In the demo, we should expect the text embeddings to automatically organize the pages into similar groups.
			- Since we're organizing notes in 3D space, they don't have to belong to rigid groups, notes can be "in between groups"
			- For example, the Wikipedia article "Philosophy of science" appears in between the group of articles on science and the group of articles on "philosophy"
			-
		- Additionally, modern user interface design has hardly explored building user interfaces in 3D space. Computers are now capable of displaying complex 3D scenes in the browser, but almost all apps are 2d only.
		- Enter 3D organization of notes using text embeddings—an approach that leverages machine learning to structure your notes in a spatially meaningful way.
	- initial draft
	  collapsed:: true
		- Organizing knowledge in 3D
		- As someone who likes reading, learning, and taking notes, I sometimes struggle on how to categorize a note
		- Categorizing a piece of information is a surprisingly difficult task.
		-
		- Organizing information in a three-dimensional landscape becomes feasible with the utilization of modern text embeddings and UMAP (Uniform Manifold Approximation and Projection) algorithms. These computer science methods allow for modeling complex structures and relationships in ways that are holistic, adaptable, and incredibly informative, making it easier to see patterns, outliers, and unexpected connections. 
		  
		  Getting into the bones of it, text embeddings are the beating heart of this new approach to knowledge organization. They are a way to convert text into numerical form, enabling computers to "understand" and process it. As texts are naturally unstructured and computers only comprehend structured data, transformation is mandatory.
		  
		  From a broader perspective, a text embedding can be considered a form of word representation that bridges the human understanding of language to that of a machine. In other words, text embeddings translate a word from its textual form into a vector in a multi-dimensional space so that the computer can work with it.
		  
		  The significance of text embeddings method lies in its ability to capture the context and semantic meaning of words. For example, the words 'king' and 'queen' would be embedded close together as they're both related to royal authority. The spatial relation in text embeddings not only captures the contextual similarity but also the relational meanings. Hence, the transition from 'king' to 'queen' might be parallel to the transition from 'man' to 'woman' in the vector space.
		  
		  UMAP, standing for Uniform Manifold Approximation and Projection, is a dimension reduction technique. It is like a more advanced version of Principal Component Analysis (PCA). UMAP can reduce the dimensionality of the text embeddings vector space, which might originally be several hundreds of dimensions, into a 2D or 3D visualization. Such a model lets one see the global and local structure of the data simultaneously, essentially allowing to see how words with similar meanings tend to group together while understand the broader context of different such groups.
		  
		  Altogether, the integration of text embeddings and UMAP provides a practical pipeline for mapping the multi-dimensional similarities in a group of texts — like a set of notes or articles — into a three-dimensional landscape that organizes this knowledge according to the inherent, sometimes subtle, relationships that underpin it. By exploring this 3D landscape, one could effectively navigate through a complex information network, a process that, hopefully, makes learning more manageable, exciting and insightful.